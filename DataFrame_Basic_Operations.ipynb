{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Operations\n",
    "\n",
    "This lecture will cover some basic operations with Spark DataFrames.\n",
    "\n",
    "We will play around with some stock data from Apple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create SparkSession object is the first entrypoint to start your Spark code. For each time running Spark job, the SparkDriver base on SparkSession to divide task, assign task to worker and collect result from worker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/08/15 10:11:16 WARN Utils: Your hostname, quangnx7-Inspiron-16-Plus-7630 resolves to a loopback address: 127.0.1.1; using 192.168.1.13 instead (on interface wlp0s20f3)\n",
      "24/08/15 10:11:16 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/08/15 10:11:16 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# May take awhile locally\n",
    "spark = SparkSession.builder.appName(\"DemoOperations\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Let Spark know about the header and infer the Schema types!\n",
    "df = spark.read.csv(\"appl_stock.csv\", inferSchema=True, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print schema\n",
    "\n",
    "When you read csv with `inferSchem=True`, you can view the schema automatically assign to the Spark data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Date: date (nullable = true)\n",
      " |-- Open: double (nullable = true)\n",
      " |-- High: double (nullable = true)\n",
      " |-- Low: double (nullable = true)\n",
      " |-- Close: double (nullable = true)\n",
      " |-- Volume: integer (nullable = true)\n",
      " |-- Adj Close: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Filtering Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter Close > 500\n",
    "\n",
    "Remember Spark is lazy, so it does not run until you call an action(). The code below, show() is an action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+------------------+------------------+------------------+---------+-----------------+\n",
      "|      Date|              Open|              High|               Low|             Close|   Volume|        Adj Close|\n",
      "+----------+------------------+------------------+------------------+------------------+---------+-----------------+\n",
      "|2012-02-13|        499.529991|503.83000899999996|497.08998899999995|502.60002099999997|129304000|        65.116633|\n",
      "|2012-02-14|        504.659988|         509.56002|        502.000008|        509.459991|115099600|        66.005408|\n",
      "|2012-02-16|        491.500008|        504.890007|         486.62999|502.20999900000004|236138000|        65.066102|\n",
      "|2012-02-17|        503.109993|507.77002000000005|        500.299995|         502.12001|133951300|        65.054443|\n",
      "|2012-02-21|506.88001299999996|        514.850021|504.12000300000005|        514.850021|151398800|        66.703738|\n",
      "|2012-02-22|        513.079994|        515.489983|509.07002300000005|        513.039993|120825600|66.46923100000001|\n",
      "|2012-02-23|        515.079987|        517.830009|        509.499992| 516.3899769999999|142006900|        66.903253|\n",
      "|2012-02-24| 519.6699980000001|        522.899979| 518.6400150000001| 522.4099809999999|103768000|        67.683203|\n",
      "|2012-02-27|        521.309982|             528.5| 516.2800139999999|        525.760017|136895500|        68.117232|\n",
      "|2012-02-28|        527.960014|        535.410011|        525.850006|        535.410011|150096800|69.36748100000001|\n",
      "|2012-02-29| 541.5600049999999| 547.6100230000001|        535.700005|        542.440025|238002800|        70.278286|\n",
      "|2012-03-01|        548.169983|        548.209984| 538.7699809999999| 544.4699780000001|170817500|        70.541286|\n",
      "|2012-03-02|        544.240013|        546.800018|        542.519974|        545.180008|107928100|        70.633277|\n",
      "|2012-03-05|        545.420013|         547.47998|        526.000023| 533.1600269999999|202281100|        69.075974|\n",
      "|2012-03-06|        523.659996|        533.690025| 516.2199860000001|        530.259987|202559700|68.70024599999999|\n",
      "|2012-03-07| 536.8000030000001|        537.779999|        523.299988| 530.6900099999999|199630200|68.75595899999999|\n",
      "|2012-03-08| 534.6899950000001|        542.989998|        532.120003|        541.989975|129114300|        70.219978|\n",
      "|2012-03-09|        544.209999|        547.740013|        543.110001|        545.170021|104729800|70.63198299999999|\n",
      "|2012-03-12| 548.9799879999999|        551.999977|        547.000023|        551.999977|101820600|        71.516869|\n",
      "|2012-03-13|        557.540024|            568.18|        555.750023|        568.099998|172713800|73.60278100000001|\n",
      "+----------+------------------+------------------+------------------+------------------+---------+-----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(\"Close>500\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Filter Close and select"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|              Open|\n",
      "+------------------+\n",
      "|        499.529991|\n",
      "|        504.659988|\n",
      "|        491.500008|\n",
      "|        503.109993|\n",
      "|506.88001299999996|\n",
      "|        513.079994|\n",
      "|        515.079987|\n",
      "| 519.6699980000001|\n",
      "|        521.309982|\n",
      "|        527.960014|\n",
      "| 541.5600049999999|\n",
      "|        548.169983|\n",
      "|        544.240013|\n",
      "|        545.420013|\n",
      "|        523.659996|\n",
      "| 536.8000030000001|\n",
      "| 534.6899950000001|\n",
      "|        544.209999|\n",
      "| 548.9799879999999|\n",
      "|        557.540024|\n",
      "+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(\"Close>500\").select(\"Open\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+------------------+\n",
      "|              Open|             Close|\n",
      "+------------------+------------------+\n",
      "|        499.529991|502.60002099999997|\n",
      "|        504.659988|        509.459991|\n",
      "|        491.500008|502.20999900000004|\n",
      "|        503.109993|         502.12001|\n",
      "|506.88001299999996|        514.850021|\n",
      "|        513.079994|        513.039993|\n",
      "|        515.079987| 516.3899769999999|\n",
      "| 519.6699980000001| 522.4099809999999|\n",
      "|        521.309982|        525.760017|\n",
      "|        527.960014|        535.410011|\n",
      "| 541.5600049999999|        542.440025|\n",
      "|        548.169983| 544.4699780000001|\n",
      "|        544.240013|        545.180008|\n",
      "|        545.420013| 533.1600269999999|\n",
      "|        523.659996|        530.259987|\n",
      "| 536.8000030000001| 530.6900099999999|\n",
      "| 534.6899950000001|        541.989975|\n",
      "|        544.209999|        545.170021|\n",
      "| 548.9799879999999|        551.999977|\n",
      "|        557.540024|        568.099998|\n",
      "+------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(\"Close>500\").select([\"Open\", \"Close\"]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using normal python comparison operators is another way to do this, they will look very similar to SQL operators, except you need to make sure you are calling the entire column within the dataframe, using the format: df[\"column name\"]\n",
    "\n",
    "Let's see some examples:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using\n",
    "\n",
    "\n",
    "& : and\n",
    "\n",
    "| : or\n",
    "\n",
    "~ : not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+------------------+------------------+------------------+---------+------------------+\n",
      "|      Date|              Open|              High|               Low|             Close|   Volume|         Adj Close|\n",
      "+----------+------------------+------------------+------------------+------------------+---------+------------------+\n",
      "|2010-01-22|206.78000600000001|        207.499996|            197.16|            197.75|220441900|         25.620401|\n",
      "|2010-01-28|        204.930004|        205.500004|        198.699995|        199.289995|293375600|25.819922000000002|\n",
      "|2010-01-29|        201.079996|        202.199995|        190.250002|        192.060003|311488100|         24.883208|\n",
      "|2010-02-01|192.36999699999998|             196.0|191.29999899999999|        194.729998|187469100|         25.229131|\n",
      "|2010-02-02|        195.909998|        196.319994|193.37999299999998|        195.859997|174585600|25.375532999999997|\n",
      "|2010-02-03|        195.169994|        200.200003|        194.420004|        199.229994|153832000|25.812148999999998|\n",
      "|2010-02-04|        196.730003|        198.370001|        191.570005|        192.050003|189413000|         24.881912|\n",
      "|2010-02-05|192.63000300000002|             196.0|        190.850002|        195.460001|212576700|25.323710000000002|\n",
      "|2010-02-08|        195.690006|197.88000300000002|        193.999994|194.11999699999998|119567700|           25.1501|\n",
      "|2010-02-09|        196.419996|        197.499994|        194.749998|196.19000400000002|158221700|         25.418289|\n",
      "|2010-02-10|        195.889997|             196.6|            194.26|195.12000700000002| 92590400|          25.27966|\n",
      "|2010-02-11|        194.880001|        199.750006|194.05999599999998|        198.669994|137586400|         25.739595|\n",
      "|2010-02-23|        199.999998|        201.330002|        195.709993|        197.059998|143773700|         25.531005|\n",
      "|2014-08-20|        100.440002|        101.089996|         99.949997|            100.57| 52699000| 95.89950999999999|\n",
      "|2014-08-21|            100.57|        100.940002|        100.110001|100.58000200000001| 33478000|         95.909048|\n",
      "|2014-08-22|        100.290001|        101.470001|        100.190002|            101.32| 44184000|          96.61468|\n",
      "|2014-08-25|        101.790001|102.16999799999999|        101.279999|        101.540001| 40270000|         96.824465|\n",
      "|2014-08-26|101.41999799999999|             101.5|        100.860001|        100.889999| 33152000|         96.204649|\n",
      "|2014-08-27|        101.019997|            102.57|        100.699997|        102.129997| 52369000|         97.387061|\n",
      "|2014-08-28|        101.589996|        102.779999|        101.559998|            102.25| 68460000|         97.501491|\n",
      "+----------+------------------+------------------+------------------+------------------+---------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter((df[\"Open\"] > 100) & (df[\"Close\"] < 200)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+------------------+------------------+------------------+---------+------------------+\n",
      "|      Date|              Open|              High|               Low|             Close|   Volume|         Adj Close|\n",
      "+----------+------------------+------------------+------------------+------------------+---------+------------------+\n",
      "|2010-01-04|        213.429998|        214.499996|212.38000099999996|        214.009998|123432400|         27.727039|\n",
      "|2010-01-05|        214.599998|        215.589994|        213.249994|        214.379993|150476200|27.774976000000002|\n",
      "|2010-01-06|        214.379993|            215.23|        210.750004|        210.969995|138040000|27.333178000000004|\n",
      "|2010-01-07|            211.75|        212.000006|        209.050005|            210.58|119282800|          27.28265|\n",
      "|2010-01-08|        210.299994|        212.000006|209.06000500000002|211.98000499999998|111902700|         27.464034|\n",
      "|2010-01-11|212.79999700000002|        213.000002|        208.450005|210.11000299999998|115557400|         27.221758|\n",
      "|2010-01-12|209.18999499999998|209.76999500000002|        206.419998|        207.720001|148614900|          26.91211|\n",
      "|2010-01-13|        207.870005|210.92999500000002|        204.099998|        210.650002|151473000|          27.29172|\n",
      "|2010-01-14|210.11000299999998|210.45999700000002|        209.020004|            209.43|108223500|         27.133657|\n",
      "|2010-01-15|210.92999500000002|211.59999700000003|        205.869999|            205.93|148516900|26.680197999999997|\n",
      "|2010-01-19|        208.330002|215.18999900000003|        207.240004|        215.039995|182501900|27.860484999999997|\n",
      "|2010-01-20|        214.910006|        215.549994|        209.500002|            211.73|153038200|         27.431644|\n",
      "|2010-01-21|        212.079994|213.30999599999998|        207.210003|        208.069996|152038600|         26.957455|\n",
      "|2010-01-25|202.51000200000001|        204.699999|        200.190002|        203.070002|266424900|26.309658000000002|\n",
      "|2010-01-26|205.95000100000001|        213.710005|        202.580004|        205.940001|466777500|         26.681494|\n",
      "|2010-01-27|        206.849995|            210.58|        199.530001|        207.880005|430642100|26.932840000000002|\n",
      "|2010-02-12|        198.109995|        201.639996|        195.500002|200.37999299999998|163867200|25.961142000000002|\n",
      "|2010-02-16|        201.940002|        203.690002|        201.520006|        203.399996|135934400|         26.352412|\n",
      "|2010-02-17|        204.190001|        204.310003|        200.860004|        202.550003|109099200|         26.242287|\n",
      "|2010-02-18|        201.629995|        203.889994|        200.920006|        202.929998|105706300|         26.291519|\n",
      "+----------+------------------+------------------+------------------+------------------+---------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter((df[\"Open\"] < 100) | ~(df[\"Close\"] < 200)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+----------+----------+------+---------+---------+\n",
      "|      Date|  Open|      High|       Low| Close|   Volume|Adj Close|\n",
      "+----------+------+----------+----------+------+---------+---------+\n",
      "|2010-01-07|211.75|212.000006|209.050005|210.58|119282800| 27.28265|\n",
      "+----------+------+----------+----------+------+---------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(df[\"Close\"] == 210.58).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "collect() is action to return Spark DataFrame as a list of Spark rows. You can use this list of Spark rows to expose data to another third paries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = df.filter(df[\"Close\"] == 210.58).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Date=datetime.date(2010, 1, 7), Open=211.75, High=212.000006, Low=209.050005, Close=210.58, Volume=119282800, Adj Close=27.28265)]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.types.Row"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(result[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Date': datetime.date(2010, 1, 7),\n",
       " 'Open': 211.75,\n",
       " 'High': 212.000006,\n",
       " 'Low': 209.050005,\n",
       " 'Close': 210.58,\n",
       " 'Volume': 119282800,\n",
       " 'Adj Close': 27.28265}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[0].asDict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2010-01-07\n",
      "211.75\n",
      "212.000006\n",
      "209.050005\n",
      "210.58\n",
      "119282800\n",
      "27.28265\n"
     ]
    }
   ],
   "source": [
    "for item in result[0]:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enrich schema\n",
    "\n",
    "In Spark we can assign schema to data frame when read data to some specific semi-structure data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = spark.read.json(\"people.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- age: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+\n",
      "| age|   name|\n",
      "+----+-------+\n",
      "|null|Michael|\n",
      "|  30|   Andy|\n",
      "|  19| Justin|\n",
      "+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You have to\n",
    "\n",
    "- Define StructField with corresponding data type, e.g: StringType, IntegerType\n",
    "- Group all StructField into list, then add this list to StructType\n",
    "- Add StrucType to the SparkReader with parameter `schema`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructField, StringType, IntegerType, StructType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_schema = [StructField(\"age\", IntegerType(), True), StructField(\"name\", StringType(), True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_type = StructType(fields=data_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = spark.read.json(\"/home/quangnx7/Workspace/Study/Fresher_DE/Spark_DataFrames/people.json\", schema=final_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- age: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark Aggregate\n",
    "\n",
    "You can aggregate data by using `groupBy()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = spark.read.csv(\"/home/quangnx7/Workspace/Study/Fresher_DE/Spark_DataFrames/sales_info.csv\", inferSchema=True, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Company: string (nullable = true)\n",
      " |-- Person: string (nullable = true)\n",
      " |-- Sales: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df3.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+-----+\n",
      "|Company| Person|Sales|\n",
      "+-------+-------+-----+\n",
      "|   GOOG|    Sam|200.0|\n",
      "|   GOOG|Charlie|120.0|\n",
      "|   GOOG|  Frank|340.0|\n",
      "|   MSFT|   Tina|600.0|\n",
      "|   MSFT|    Amy|124.0|\n",
      "|   MSFT|Vanessa|243.0|\n",
      "|     FB|   Carl|870.0|\n",
      "|     FB|  Sarah|350.0|\n",
      "|   APPL|   John|250.0|\n",
      "|   APPL|  Linda|130.0|\n",
      "|   APPL|   Mike|750.0|\n",
      "|   APPL|  Chris|350.0|\n",
      "+-------+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.group.GroupedData at 0x727f8d334c70>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3.groupBy(\"Company\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+\n",
      "|Company|       avg(Sales)|\n",
      "+-------+-----------------+\n",
      "|   APPL|            370.0|\n",
      "|   GOOG|            220.0|\n",
      "|     FB|            610.0|\n",
      "|   MSFT|322.3333333333333|\n",
      "+-------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df3.groupBy(\"Company\").mean().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import format_number\n",
    "from pyspark.sql.functions import count_distinct, stddev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------+\n",
      "|format_number(std_sales, 2)|\n",
      "+---------------------------+\n",
      "|                     250.09|\n",
      "+---------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df3_std = df3.select(stddev(\"sales\").alias(\"std_sales\"))\n",
    "df3_std = df3_std.select(format_number(\"std_sales\", 2)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SparkSQL\n",
    "\n",
    "You can transform data by the form of SQL syntax by using SparkSQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.createOrReplaceTempView(\"sales_info\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_stmt = \"\"\"select Company, Person, Sales as Max_sales\n",
    "from (\n",
    "    select Company, Person, Sales,\n",
    "        dense_rank() over(partition by Company order by Sales desc) as rank_sales\n",
    "    from sales_info\n",
    ")\n",
    "where rank_sales = 1\n",
    "\"\"\"\n",
    "\n",
    "sql_result = spark.sql(sql_stmt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Company: string, Person: string, Max_sales: double]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sql_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+---------+\n",
      "|Company|Person|Max_sales|\n",
      "+-------+------+---------+\n",
      "|   APPL|  Mike|    750.0|\n",
      "|     FB|  Carl|    870.0|\n",
      "|   GOOG| Frank|    340.0|\n",
      "|   MSFT|  Tina|    600.0|\n",
      "+-------+------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sql_result.show()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
